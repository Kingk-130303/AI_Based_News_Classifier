{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for to store data\n",
    "\n",
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working folder for the experiment (old model)\n",
    "BASE_DIR = './E1'\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "print(\n",
    "    f'base directory: {BASE_DIR}\\n\\n'\n",
    "    f'data: {data_dir}\\n'\n",
    "    f'model: {model_dir}\\n'\n",
    "    f'vocab: {vocab_dir}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column width so you can see the entire length of the `title` column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the datasets into dataframes\n",
    "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "\n",
    "# Preview the first 10 rows of the training set\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['title', 'topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_compile_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENTERTAINMENT',\n",
       " 'HEALTH',\n",
       " 'TECHNOLOGY',\n",
       " 'WORLD',\n",
       " 'BUSINESS',\n",
       " 'SPORTS',\n",
       " 'NATION',\n",
       " 'SCIENCE']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab_dir}/labels.txt', num_oov_indices=0)\n",
    "\n",
    "# Check the list of labels\n",
    "topic_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Sample text: Dengue fever cases in Laos rise to 4256\n",
      "WARNING:tensorflow:From i:\\ML_Projects\\AI_Based_News_Classifier\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Sample text (preprocessed): [4040 1979   30    2 9339  282    3    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Title length and vocabulary size used by the team for the prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "vocab_path = f'{vocab_dir}/vocabulary.txt'\n",
    "with open(vocab_path, 'r', encoding='utf-8') as file:\n",
    "    vocab = [line.strip() for line in file]\n",
    "\n",
    "# Set the vocabulary for the TextVectorization layer\n",
    "title_preprocessor.set_vocabulary(vocab)\n",
    "\n",
    "# Check the vocabulary size\n",
    "print(f'Vocabulary size: {title_preprocessor.vocabulary_size()}')\n",
    "\n",
    "# Get a sample title\n",
    "sample_title = train_df['title'][10]\n",
    "\n",
    "# Sample title in string format\n",
    "print(f\"Sample text: {sample_title}\")\n",
    "\n",
    "# Sample title represented as an integer sequence\n",
    "print(f\"Sample text (preprocessed): {title_preprocessor(sample_title).numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test dataframe to a tf dataset\n",
    "test_ds = lab_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Get the metrics\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of each class in the train set\n",
    "train_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of each class in the test set\n",
    "test_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test sets\n",
    "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "\n",
    "# Combine the two datasets. Set ignore_index to False.\n",
    "combined_df = pd.concat([train_df,test_df], ignore_index=True)\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['topic'])\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25, stratify=train_df['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory for the second experiment\n",
    "BASE_DIR = './E2'\n",
    "\n",
    "# Set the subdirectories that will contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Save the datasets\n",
    "lab_utils.save_data(train_df, data_dir, 'train_data.csv')\n",
    "lab_utils.save_data(dev_df, data_dir, 'dev_data.csv')\n",
    "lab_utils.save_data(test_df, data_dir, 'test_data.csv')\n",
    "\n",
    "# Save the labels\n",
    "lab_utils.save_labels(topic_lookup, vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
